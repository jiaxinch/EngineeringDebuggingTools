{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train CIFAR10 with PyTorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import easydict\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from models import *\n",
    "from utils import progress_bar\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_norm_by_param(model):\n",
    "    return {name: torch.sum(param ** 2) for name, param in model.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_norm(model):\n",
    "    grad_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grad_norm += (p.grad.data ** 2).sum().item()\n",
    "    return grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_mean(model):\n",
    "    grad_mean = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grad_mean += p.grad.data.mean().item()\n",
    "    return grad_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_mean(model):\n",
    "    grad_mean = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            grad_mean += p.grad.data.std().item()\n",
    "    return grad_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n",
    "#parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "#parser.add_argument('--resume', '-r', action='store_true', help='resume from checkpoint')\n",
    "#args = parser.parse_args(argv[1:])\n",
    "args = easydict.EasyDict({\n",
    "    \"lr\": 0.1,\n",
    "    \"resume\": '-r'\n",
    "})\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(model, epoch):\n",
    "    writer.add_scalar('Gradient norm', compute_gradient_norm(model), epoch)\n",
    "    writer.add_scalar('Gradient mean', compute_gradient_mean(model), epoch)\n",
    "    writer.add_scalar('Gradient std', compute_gradient_mean(model), epoch)\n",
    "    diction = get_grad_norm_by_param(model)\n",
    "    for layer in diction:\n",
    "        writer.add_scalar(layer, diction[layer], epoch)\n",
    "    \n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             flat=torch.flatten(p.grad.data)\n",
    "#             print(flat.nelement())\n",
    "#             for num in flat:\n",
    "                #print(num.item())\n",
    "                #writer.add_histogram('Gradient', num, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "print('==> Building model..')\n",
    "# net = VGG('VGG19')\n",
    "net = ResNet18()\n",
    "# net = PreActResNet18()\n",
    "# net = GoogLeNet()\n",
    "# net = DenseNet121()\n",
    "# net = ResNeXt29_2x64d()\n",
    "# net = MobileNet()\n",
    "# net = MobileNetV2()\n",
    "# net = DPN92()\n",
    "# net = ShuffleNetG2()\n",
    "# net = SENet18()\n",
    "# net = ShuffleNetV2(1)\n",
    "#net = EfficientNetB0()\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "# if args.resume:\n",
    "#     # Load checkpoint.\n",
    "#     print('==> Resuming from checkpoint..')\n",
    "#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "#     checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "#     net.load_state_dict(checkpoint['net'])\n",
    "#     best_acc = checkpoint['acc']\n",
    "#     start_epoch = checkpoint['epoch']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    visualize(net, epoch)\n",
    "    hist_loss = np.array([])\n",
    "    hist_accuracy = np.array([])\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        hist_loss = np.append(hist_loss, train_loss/(batch_idx+1))\n",
    "        hist_accuracy = np.append(hist_accuracy, 100.*correct/total)\n",
    "        \n",
    "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "    writer.add_histogram('train loss', hist_loss, epoch)\n",
    "    writer.add_histogram('train accuracy', hist_accuracy, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "             # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      " [================================================================>]  Step: 112ms | Tot: 57s182ms | Loss: 1.973 | Acc: 29.136% (14568/5000 391/391 \n",
      " [================================================================>]  Step: 38ms | Tot: 4s79ms | Loss: 1.591 | Acc: 41.480% (4148/1000 100/100 ]  Step: 39ms | Tot: 174ms | Loss: 1.584 | Acc: 40.600% (203/50 5/100 .............................]  Step: 43ms | Tot: 346ms | Loss: 1.599 | Acc: 41.222% (371/90 9/10 10/100 11/100 =======>.........................................................]  Step: 38ms | Tot: 475ms | Loss: 1.593 | Acc: 41.750% (501/120 12/10 13/100 ==========>......................................................]  Step: 37ms | Tot: 672ms | Loss: 1.598 | Acc: 41.588% (707/170 17/100 .......................]  Step: 42ms | Tot: 845ms | Loss: 1.598 | Acc: 41.857% (879/210 21/100 30/100 ====================>............................................]  Step: 37ms | Tot: 1s339ms | Loss: 1.593 | Acc: 41.697% (1376/330 33/100 =======================>.........................................]  Step: 40ms | Tot: 1s506ms | Loss: 1.593 | Acc: 41.568% (1538/370 37/10 42/100 =================================>...............................]  Step: 40ms | Tot: 2s158ms | Loss: 1.583 | Acc: 41.736% (2212/530 53/100 ===================================>.............................]  Step: 36ms | Tot: 2s235ms | Loss: 1.586 | Acc: 41.691% (2293/550 55/100 ===================================>.............................]  Step: 37ms | Tot: 2s273ms | Loss: 1.587 | Acc: 41.589% (2329/560 56/100 ============================>............................]  Step: 43ms | Tot: 2s317ms | Loss: 1.589 | Acc: 41.561% (2369/570 57/100 =================================>.........................]  Step: 38ms | Tot: 2s514ms | Loss: 1.588 | Acc: 41.516% (2574/620 62/100 ========================================>........................]  Step: 42ms | Tot: 2s598ms | Loss: 1.587 | Acc: 41.516% (2657/640 64/100 ===================================>.....................]  Step: 39ms | Tot: 2s769ms | Loss: 1.590 | Acc: 41.456% (2819/680 68/100 ........]  Step: 45ms | Tot: 2s814ms | Loss: 1.590 | Acc: 41.435% (2859/690 69/100 70/100 ==================>..................]  Step: 39ms | Tot: 2s942ms | Loss: 1.592 | Acc: 41.431% (2983/720 72/100 ==================================================>..............]  Step: 41ms | Tot: 3s233ms | Loss: 1.587 | Acc: 41.722% (3296/790 79/100 ====================================================>..........]  Step: 37ms | Tot: 3s470ms | Loss: 1.590 | Acc: 41.612% (3537/850 85/100 =======================================================>.........]  Step: 39ms | Tot: 3s510ms | Loss: 1.590 | Acc: 41.581% (3576/860 86/100 =======================================================>.........]  Step: 41ms | Tot: 3s552ms | Loss: 1.591 | Acc: 41.598% (3619/870 87/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 1\n",
      " [================================================================>]  Step: 105ms | Tot: 58s230ms | Loss: 1.516 | Acc: 43.982% (21991/5000 391/391 \n",
      " [================================================================>]  Step: 45ms | Tot: 4s203ms | Loss: 1.420 | Acc: 47.520% (4752/1000 100/100 ..........................................................]  Step: 44ms | Tot: 179ms | Loss: 1.405 | Acc: 49.000% (245/50 5/100 ....................................................]  Step: 41ms | Tot: 642ms | Loss: 1.427 | Acc: 47.812% (765/160 16/100 ....]  Step: 45ms | Tot: 688ms | Loss: 1.426 | Acc: 48.118% (818/170 17/100 ..................................]  Step: 35ms | Tot: 970ms | Loss: 1.420 | Acc: 48.125% (1155/240 24/100 =>........................................]  Step: 43ms | Tot: 1s596ms | Loss: 1.425 | Acc: 48.256% (1882/390 39/100 .........]  Step: 49ms | Tot: 2s189ms | Loss: 1.420 | Acc: 47.906% (2539/530 53/10 54/100 ===================================>.............................]  Step: 36ms | Tot: 2s269ms | Loss: 1.425 | Acc: 47.745% (2626/550 55/10 56/100 58/10 59/100 ======================================>..........................]  Step: 41ms | Tot: 2s491ms | Loss: 1.423 | Acc: 47.650% (2859/600 60/100 =====================================>.........................]  Step: 42ms | Tot: 2s534ms | Loss: 1.422 | Acc: 47.639% (2906/610 61/100 ========================================>........................]  Step: 41ms | Tot: 2s658ms | Loss: 1.422 | Acc: 47.641% (3049/640 64/10 66/100 ================================>....................]  Step: 41ms | Tot: 2s876ms | Loss: 1.423 | Acc: 47.536% (3280/690 69/100 =============================================>...................]  Step: 41ms | Tot: 2s958ms | Loss: 1.425 | Acc: 47.549% (3376/710 71/100 ==================================>..................]  Step: 42ms | Tot: 3s1ms | Loss: 1.424 | Acc: 47.528% (3422/720 72/100 ==============================================>..................]  Step: 50ms | Tot: 3s52ms | Loss: 1.423 | Acc: 47.575% (3473/730 73/10 74/100 .......]  Step: 38ms | Tot: 3s172ms | Loss: 1.420 | Acc: 47.618% (3619/760 76/100 78/100 ===================================================>.............]  Step: 43ms | Tot: 3s350ms | Loss: 1.421 | Acc: 47.575% (3806/800 80/100 ====================================================>............]  Step: 42ms | Tot: 3s393ms | Loss: 1.419 | Acc: 47.667% (3861/810 81/100 ==>............]  Step: 44ms | Tot: 3s438ms | Loss: 1.418 | Acc: 47.671% (3909/820 82/100 83/100 ==================================================>.........]  Step: 41ms | Tot: 3s610ms | Loss: 1.419 | Acc: 47.628% (4096/860 86/10 87/10 88/100 =========================================================>.......]  Step: 40ms | Tot: 3s742ms | Loss: 1.421 | Acc: 47.528% (4230/890 89/100 =========================================================>.......]  Step: 47ms | Tot: 3s789ms | Loss: 1.421 | Acc: 47.433% (4269/900 90/100 ==========================================================>......]  Step: 38ms | Tot: 3s828ms | Loss: 1.420 | Acc: 47.451% (4318/910 91/100 \n",
      "Saving..\n",
      "\n",
      "Epoch: 2\n",
      " [================================================================>]  Step: 107ms | Tot: 59s69ms | Loss: 1.250 | Acc: 54.362% (27181/5000 391/391  46/391 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 38ms | Tot: 4s153ms | Loss: 1.188 | Acc: 57.300% (5730/1000 100/100 ..........................................................]  Step: 44ms | Tot: 191ms | Loss: 1.142 | Acc: 59.200% (296/50 5/100 .....................................................]  Step: 40ms | Tot: 232ms | Loss: 1.108 | Acc: 59.833% (359/60 6/100 ===>.............................................................]  Step: 42ms | Tot: 275ms | Loss: 1.119 | Acc: 59.714% (418/70 7/100 ................................]  Step: 42ms | Tot: 318ms | Loss: 1.146 | Acc: 58.375% (467/80 8/100 ...........]  Step: 46ms | Tot: 365ms | Loss: 1.160 | Acc: 58.333% (525/90 9/100 .............................]  Step: 40ms | Tot: 490ms | Loss: 1.161 | Acc: 57.917% (695/120 12/100 14/100 =========>.......................................................]  Step: 47ms | Tot: 626ms | Loss: 1.163 | Acc: 57.800% (867/150 15/100 =========>.......................................................]  Step: 43ms | Tot: 670ms | Loss: 1.177 | Acc: 57.188% (915/160 16/100 ...............................................]  Step: 42ms | Tot: 713ms | Loss: 1.179 | Acc: 57.118% (971/170 17/100 ..................................]  Step: 42ms | Tot: 756ms | Loss: 1.181 | Acc: 56.833% (1023/180 18/100 20/100 21/100 ==========>...................................................]  Step: 38ms | Tot: 932ms | Loss: 1.194 | Acc: 57.136% (1257/220 22/100  24/10 25/100 26/100 =========>................................................]  Step: 39ms | Tot: 1s150ms | Loss: 1.199 | Acc: 57.074% (1541/270 27/100 ...............................................]  Step: 42ms | Tot: 1s193ms | Loss: 1.200 | Acc: 57.036% (1597/280 28/100 29/100 .......]  Step: 47ms | Tot: 1s365ms | Loss: 1.188 | Acc: 57.531% (1841/320 32/100 ====================>............................................]  Step: 35ms | Tot: 1s401ms | Loss: 1.186 | Acc: 57.636% (1902/330 33/100 ==========================>......................................]  Step: 37ms | Tot: 1s721ms | Loss: 1.182 | Acc: 57.732% (2367/410 41/100 ===========================>.....................................]  Step: 45ms | Tot: 1s809ms | Loss: 1.184 | Acc: 57.814% (2486/430 43/100 =======================>................................]  Step: 42ms | Tot: 2s148ms | Loss: 1.175 | Acc: 57.843% (2950/510 51/100   Step: 39ms | Tot: 2s271ms | Loss: 1.177 | Acc: 57.741% (3118/540 54/100 59/10 62/100  66/100 ============================================>....................]  Step: 41ms | Tot: 2s903ms | Loss: 1.182 | Acc: 57.464% (3965/690 69/100 ======================================>..................]  Step: 40ms | Tot: 3s24ms | Loss: 1.184 | Acc: 57.389% (4132/720 72/100 74/100 =================================================>..............]  Step: 36ms | Tot: 3s311ms | Loss: 1.180 | Acc: 57.266% (4524/790 79/100 =============================================>.............]  Step: 36ms | Tot: 3s347ms | Loss: 1.180 | Acc: 57.237% (4579/800 80/100 ==========================================>...........]  Step: 39ms | Tot: 3s469ms | Loss: 1.180 | Acc: 57.277% (4754/830 83/100 ====================================>.........]  Step: 35ms | Tot: 3s586ms | Loss: 1.181 | Acc: 57.407% (4937/860 86/100 ================================================>.........]  Step: 38ms | Tot: 3s625ms | Loss: 1.183 | Acc: 57.333% (4988/870 87/100 =========================================================>.......]  Step: 36ms | Tot: 3s705ms | Loss: 1.185 | Acc: 57.326% (5102/890 89/10 93/100 =============================================================>...]  Step: 37ms | Tot: 3s952ms | Loss: 1.185 | Acc: 57.442% (5457/950 95/100 ==============================================================>..]  Step: 37ms | Tot: 4s36ms | Loss: 1.185 | Acc: 57.412% (5569/970 97/100 =====================>.]  Step: 36ms | Tot: 4s114ms | Loss: 1.187 | Acc: 57.323% (5675/990 99/100 \n",
      "Saving..\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, start_epoch+3):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1728\n",
      "64\n",
      "64\n",
      "36864\n",
      "64\n",
      "64\n",
      "36864\n",
      "64\n",
      "64\n",
      "36864\n",
      "64\n",
      "64\n",
      "36864\n",
      "64\n",
      "64\n",
      "73728\n",
      "128\n",
      "128\n",
      "147456\n",
      "128\n",
      "128\n",
      "8192\n",
      "128\n",
      "128\n",
      "147456\n",
      "128\n",
      "128\n",
      "147456\n",
      "128\n",
      "128\n",
      "294912\n",
      "256\n",
      "256\n",
      "589824\n",
      "256\n",
      "256\n",
      "32768\n",
      "256\n",
      "256\n",
      "589824\n",
      "256\n",
      "256\n",
      "589824\n",
      "256\n",
      "256\n",
      "1179648\n",
      "512\n",
      "512\n",
      "2359296\n",
      "512\n",
      "512\n",
      "131072\n",
      "512\n",
      "512\n",
      "2359296\n",
      "512\n",
      "512\n",
      "2359296\n",
      "512\n",
      "512\n",
      "5120\n",
      "10\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
